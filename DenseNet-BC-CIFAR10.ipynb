{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DenseNet-BC-CIFAR10.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"nG-arV36yJb8","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision.utils import save_image\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import os\n","import glob\n","import PIL\n","from PIL import Image\n","from torch.utils import data as D\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import random\n","import torchsummary"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jeYvMzm-yTWu","colab_type":"code","colab":{}},"cell_type":"code","source":["print(torch.__version__)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jsH00nhEyg4F","colab_type":"code","colab":{}},"cell_type":"code","source":["batch_size = 64\n","validation_ratio = 0.1\n","random_seed = 10\n","initial_lr = 0.1\n","num_epoch = 1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"U9OZd9XaykqQ","colab_type":"code","colab":{}},"cell_type":"code","source":["transform_train = transforms.Compose([\n","        transforms.RandomCrop(32, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n","\n","transform_validation = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n","\n","\n","transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n","\n","trainset = torchvision.datasets.CIFAR10(\n","    root='./data', train=True, download=True, transform=transform_train)\n","\n","validset = torchvision.datasets.CIFAR10(\n","    root='./data', train=True, download=True, transform=transform_validation)\n","\n","testset = torchvision.datasets.CIFAR10(\n","    root='./data', train=False, download=True, transform=transform_test)\n","\n","\n","num_train = len(trainset)\n","indices = list(range(num_train))\n","split = int(np.floor(validation_ratio * num_train))\n","\n","np.random.seed(random_seed)\n","np.random.shuffle(indices)\n","\n","train_idx, valid_idx = indices[split:], indices[:split]\n","train_sampler = SubsetRandomSampler(train_idx)\n","valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","train_loader = torch.utils.data.DataLoader(\n","    trainset, batch_size=batch_size, sampler=train_sampler, num_workers=0\n",")\n","\n","valid_loader = torch.utils.data.DataLoader(\n","    validset, batch_size=batch_size, sampler=valid_sampler, num_workers=0\n",")\n","\n","test_loader = torch.utils.data.DataLoader(\n","    testset, batch_size=batch_size, shuffle=False, num_workers=0\n",")\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7-r-IAS106N0","colab_type":"code","colab":{}},"cell_type":"code","source":["class bn_relu_conv(nn.Module):\n","    def __init__(self, nin, nout, kernel_size, stride, padding, bias=False):\n","        super(bn_relu_conv, self).__init__()\n","        self.batch_norm = nn.BatchNorm2d(nin)\n","        self.relu = nn.ReLU(True)\n","        self.conv = nn.Conv2d(nin, nout, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n","\n","    def forward(self, x):\n","        out = self.batch_norm(x)\n","        out = self.relu(out)\n","        out = self.conv(out)\n","\n","        return out"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rOJHDgZX3TxJ","colab_type":"code","colab":{}},"cell_type":"code","source":["class bottleneck_layer(nn.Sequential):\n","  def __init__(self, nin, growth_rate, drop_rate=0.2):    \n","      super(bottleneck_layer, self).__init__()\n","      \n","      self.add_module('conv_1x1', bn_relu_conv(nin=nin, nout=growth_rate*4, kernel_size=1, stride=1, padding=0, bias=False))\n","      self.add_module('conv_3x3', bn_relu_conv(nin=growth_rate*4, nout=growth_rate, kernel_size=3, stride=1, padding=1, bias=False))\n","      \n","      self.drop_rate = drop_rate\n","      \n","  def forward(self, x):\n","      bottleneck_output = super(bottleneck_layer, self).forward(x)\n","      if self.drop_rate > 0:\n","          bottleneck_output = F.dropout(bottleneck_output, p=self.drop_rate, training=self.training)\n","          \n","      bottleneck_output = torch.cat((x, bottleneck_output), 1)\n","      \n","      return bottleneck_output"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qszCjalp4qNX","colab_type":"code","colab":{}},"cell_type":"code","source":["class Transition_layer(nn.Sequential):\n","  def __init__(self, nin, theta=0.5):    \n","      super(Transition_layer, self).__init__()\n","      \n","      self.add_module('conv_1x1', bn_relu_conv(nin=nin, nout=int(nin*theta), kernel_size=1, stride=1, padding=0, bias=False))\n","      self.add_module('avg_pool_2x2', nn.AvgPool2d(kernel_size=2, stride=2, padding=0))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"S00fPblY4-ha","colab_type":"code","colab":{}},"cell_type":"code","source":["class DenseBlock(nn.Sequential):\n","  def __init__(self, nin, num_bottleneck_layers, growth_rate, drop_rate=0.2):\n","      super(DenseBlock, self).__init__()\n","                        \n","      for i in range(num_bottleneck_layers):\n","          nin_bottleneck_layer = nin + growth_rate * i\n","          self.add_module('bottleneck_layer_%d' % i, bottleneck_layer(nin=nin_bottleneck_layer, growth_rate=growth_rate, drop_rate=drop_rate))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Lcqvpn2ly3lL","colab_type":"code","colab":{}},"cell_type":"code","source":["class DenseNet(nn.Module):\n","    def __init__(self, growth_rate=12, num_layers=100, theta=0.5, drop_rate=0.2, num_classes=10):\n","        super(DenseNet, self).__init__()\n","        \n","        assert (num_layers - 4) % 6 == 0\n","        \n","        # (num_layers-4)//6 \n","        num_bottleneck_layers = (num_layers - 4) // 6\n","        \n","        # 32 x 32 x 3 --> 32 x 32 x (growth_rate*2)\n","        self.dense_init = nn.Conv2d(3, growth_rate*2, kernel_size=3, stride=1, padding=1, bias=True)\n","                \n","        # 32 x 32 x (growth_rate*2) --> 32 x 32 x [(growth_rate*2) + (growth_rate * num_bottleneck_layers)]\n","        self.dense_block_1 = DenseBlock(nin=growth_rate*2, num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n","\n","        # 32 x 32 x [(growth_rate*2) + (growth_rate * num_bottleneck_layers)] --> 16 x 16 x [(growth_rate*2) + (growth_rate * num_bottleneck_layers)]*theta\n","        nin_transition_layer_1 = (growth_rate*2) + (growth_rate * num_bottleneck_layers) \n","        self.transition_layer_1 = Transition_layer(nin=nin_transition_layer_1, theta=theta)\n","        \n","        # 16 x 16 x nin_transition_layer_1*theta --> 16 x 16 x [nin_transition_layer_1*theta + (growth_rate * num_bottleneck_layers)]\n","        self.dense_block_2 = DenseBlock(nin=int(nin_transition_layer_1*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n","\n","        # 16 x 16 x [nin_transition_layer_1*theta + (growth_rate * num_bottleneck_layers)] --> 8 x 8 x [nin_transition_layer_1*theta + (growth_rate * num_bottleneck_layers)]*theta\n","        nin_transition_layer_2 = int(nin_transition_layer_1*theta) + (growth_rate * num_bottleneck_layers) \n","        self.transition_layer_2 = Transition_layer(nin=nin_transition_layer_2, theta=theta)\n","        \n","        # 8 x 8 x nin_transition_layer_2*theta --> 8 x 8 x [nin_transition_layer_2*theta + (growth_rate * num_bottleneck_layers)]\n","        self.dense_block_3 = DenseBlock(nin=int(nin_transition_layer_2*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n","        \n","        nin_fc_layer = int(nin_transition_layer_2*theta) + (growth_rate * num_bottleneck_layers) \n","        \n","        # [nin_transition_layer_2*theta + (growth_rate * num_bottleneck_layers)] --> num_classes\n","        self.fc_layer = nn.Linear(nin_fc_layer, num_classes)\n","        \n","    def forward(self, x):\n","        dense_init_output = self.dense_init(x)\n","        \n","        dense_block_1_output = self.dense_block_1(dense_init_output)\n","        transition_layer_1_output = self.transition_layer_1(dense_block_1_output)\n","        \n","        dense_block_2_output = self.dense_block_2(transition_layer_1_output)\n","        transition_layer_2_output = self.transition_layer_2(dense_block_2_output)\n","        \n","        dense_block_3_output = self.dense_block_3(transition_layer_2_output)\n","        \n","        global_avg_pool_output = F.adaptive_avg_pool2d(dense_block_3_output, (1, 1))                \n","        global_avg_pool_output_flat = global_avg_pool_output.view(global_avg_pool_output.size(0), -1)\n","\n","        output = self.fc_layer(global_avg_pool_output_flat)\n","        \n","        return output"],"execution_count":0,"outputs":[]},{"metadata":{"id":"P_Pu_kXLPKd_","colab_type":"code","colab":{}},"cell_type":"code","source":["def DenseNetBC_100_12():\n","    return DenseNet(growth_rate=12, num_layers=100, theta=0.5, drop_rate=0.2, num_classes=10)\n","\n","def DenseNetBC_250_24():\n","    return DenseNet(growth_rate=24, num_layers=250, theta=0.5, drop_rate=0.2, num_classes=10)\n","\n","def DenseNetBC_190_40():\n","    return DenseNet(growth_rate=40, num_layers=190, theta=0.5, drop_rate=0.2, num_classes=10)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CJB5NVcYPnB9","colab_type":"code","colab":{}},"cell_type":"code","source":["net = DenseNetBC_100_12()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VnWt7hXVSIxZ","colab_type":"code","colab":{}},"cell_type":"code","source":["net.to(device)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VLA5Y5Wpfgv6","colab_type":"code","colab":{}},"cell_type":"code","source":["torchsummary.summary(net, (3, 32, 32))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OtjYUJbQPvPU","colab_type":"code","colab":{}},"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=initial_lr, momentum=0.9)\n","\n","for epoch in range(num_epoch):  \n","    if epoch == 0:\n","        lr = initial_lr\n","    elif epoch == int(num_epoch * 0.5):\n","        lr *= 0.1\n","        optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n","    elif epoch == int(num_epoch * 0.75):\n","        lr *= 0.1\n","        optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n","    \n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        running_loss += loss.item()\n","        \n","        show_period = 100\n","        if i % show_period == show_period-1:    # print every \"show_period\" mini-batches\n","            print('[%d, %5d/50000] loss: %.7f' %\n","                  (epoch + 1, (i + 1)*batch_size, running_loss / show_period))\n","            running_loss = 0.0\n","        \n","        \n","    # validation part\n","    correct = 0\n","    total = 0\n","    for i, data in enumerate(valid_loader, 0):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = net(inputs)\n","        \n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        \n","    print('[%d epoch] Accuracy of the network on the validation images: %d %%' % \n","          (epoch + 1, 100 * correct / total)\n","         )\n","\n","print('Finished Training')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uVSrYBu7SVqR","colab_type":"code","colab":{}},"cell_type":"code","source":["class_correct = list(0. for i in range(10))\n","class_total = list(0. for i in range(10))\n","\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","    for data in test_loader:\n","        images, labels = data\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = net(images)\n","        _, predicted = torch.max(outputs, 1)\n","        c = (predicted == labels).squeeze()\n","                \n","        for i in range(labels.shape[0]):\n","            label = labels[i]\n","            class_correct[label] += c[i].item()\n","            class_total[label] += 1\n","            \n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of the network on the 10000 test images: %d %%' % (\n","    100 * correct / total))            \n","            \n","for i in range(10):\n","    print('Accuracy of %5s : %2d %%' % (\n","        classes[i], 100 * class_correct[i] / class_total[i]))"],"execution_count":0,"outputs":[]}]}